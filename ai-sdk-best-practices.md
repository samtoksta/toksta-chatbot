# AI SDK Best Practices for Toksta AI Software Discovery Chat

This document outlines best practices for implementing the AI Software Discovery Chat, leveraging the Vercel AI SDK (v4+) within the specified Next.js 15 / React 19 stack, based on the project's PRD (v1.2) and AI SDK documentation.

## 1. Core AI SDK Integration

*   **Leverage `generateText`:** Use the core `generateText` function within your Vercel Edge Function (Next.js API Route) for orchestrating LLM interactions. This provides a unified API across potential future model providers.
*   **Streaming Text Responses:** Ensure text summaries generated by the LLM are streamed back to the client using the AI SDK's streaming capabilities integrated with `generateText`. This is crucial for achieving the target TTFT (â‰¤ 7s) and providing a responsive UX.
*   **Streaming Structured Data (Tool Cards):**
    *   Utilize the AI SDK's capabilities for streaming structured data alongside text. Functions like `streamObject` or potentially lower-level stream helpers combined with `generateText` are suitable for sending the top 5 tool card data (`logo_url`, `name`, `nutshell`, etc.) concurrently with the textual summary.
    *   Define clear TypeScript interfaces for the tool card data structure to ensure type safety when using structured data streaming.
*   **Model Selection:** Stick to the specified `text-embedding-3-small` for embeddings. For generation, choose an appropriate model supported by the AI SDK (e.g., from OpenAI, Anthropic, Google) that balances performance, cost, and context window size for your prompt structure.

## 2. Frontend Integration (`useChat`)

*   **Employ `useChat` Hook:** Use the AI SDK UI's `useChat` hook on the client-side (`app/chat/page.tsx` or similar) to manage chat state, handle user input, and process streamed responses (both text and structured data).
*   **Handle Structured Data:** Implement logic within your React component to parse and render the streamed tool card data received via `useChat`. The AI SDK UI provides patterns for handling custom data streams.
*   **Conversation History:** Rely on `useChat` to manage the in-session conversation history needed for follow-up questions (US3). Ensure the history (messages) is appropriately passed back to the Edge Function in subsequent requests.

## 3. Backend (Vercel Edge Function / Next.js API Route)

*   **Edge Function Optimization:**
    *   Keep the Edge Function lean. Offload heavy computation (like initial embedding generation) to the weekly batch job.
    *   Address the `tools_vectors.json` loading: Ensure it's compressed. Profile the cold start impact. Consider if strategies like splitting the file or alternative in-memory stores (if edge limitations become severe) are needed, though the PRD accepts potential initial slowness.
*   **Context Management:**
    *   Implement robust logic to construct the prompt context passed to `generateText`. This includes:
        *   The current user query.
        *   Relevant historical messages from `useChat` (for follow-up).
        *   Retrieved product facts (from in-memory search).
        *   Retrieved transcript chunks (from Supabase).
    *   Critically, implement context pruning/summarization to stay within the LLM's token limits, especially for follow-up questions (Risk: Context Window Limits). The AI SDK may offer middleware or patterns for this, but custom logic will likely be needed.
*   **Fallback Logic:** Implement the fallback search logic (PRD US1, FR-6) within the Edge Function. Conditionally fetch broader context from Supabase and adjust the prompt/processing accordingly, ensuring fallback responses are flagged.
*   **Error Handling:**
    *   Wrap AI SDK calls (`generateText`, embedding lookups, Supabase queries) in `try...catch` blocks.
    *   Utilize the AI SDK's error types (`AI_SDK_Error`) if applicable for more specific error handling.
    *   Return user-friendly error messages via the chat interface as specified in the PRD. Log detailed errors server-side (Vercel Function Logs).

## 4. Data Handling & Embeddings

*   **In-Memory Search:** Optimize the loading and querying of `tools_vectors.json`. Use efficient libraries for in-memory vector similarity search if the dataset size demands it beyond simple iteration.
*   **Supabase Interaction:** Use a performant Postgres client suitable for Edge Functions (e.g., `postgres.js`) to query the `transcript_chunks` table using `pgvector`'s cosine similarity (`<=>`). Ensure connection pooling is handled correctly within the edge environment if applicable.
*   **Embedding Consistency:** Strictly use `text-embedding-3-small` (1536 dims) for both the batch job and the query embedding in the Edge Function to ensure meaningful similarity scores.

## 5. Analytics & Monitoring

*   **Integrate Vercel Analytics:** Trigger the custom events specified in the PRD (`query_submitted`, `tool_impression`, `outbound_click`, `feedback_provided`, `fallback_triggered`) at the appropriate points in the client-side (`useChat` callbacks) and server-side (Edge Function) logic.
*   **Monitor TTFT:** Closely monitor the p95 TTFT metric in Vercel Analytics to ensure the performance goal is met and regressions are caught.
*   **Monitor SDK/LLM Errors:** Use Vercel Logs and potentially external monitoring tools to track the frequency and nature of errors originating from the AI SDK or the underlying LLM provider.

## 6. General Development Practices (React 19 / Next.js 15)

*   **Prefer RSCs:** Structure the application to use React Server Components wherever possible, minimizing 'use client' directives, especially for data fetching orchestration logic outside the main chat interaction component.
*   **Use `useActionState` / `useFormStatus`:** For any form-like interactions (though the primary chat uses `useChat`), leverage the updated React 19 hooks if needed.
*   **TypeScript:** Maintain strict TypeScript usage throughout the codebase, leveraging interfaces for data structures (tool cards, API responses). Use `satisfies` where appropriate for type validation against SDK types.

By adhering to these practices, the development team can effectively utilize the Vercel AI SDK to build a performant, maintainable, and feature-rich AI Software Discovery Chat that meets the requirements outlined in the PRD. 

In your chat component where useChat is used
const { messages, input, handleInputChange, handleSubmit } = useChat({
  api: '/api/chat',
  onFinish: (message) => {
    // Each message should already contain its cards data
    // No need for additional state management if implemented correctly
  },
});

// In your rendering logic
return (
  <div className="flex flex-col gap-4">
    {messages.map((message) => (
      <div key={message.id} className="message">
        {/* Message text */}
        <div className="message-text">{message.content}</div>
        
        {/* Tools/cards for this specific message */}
        {message.data?.cards && (
          <div className="tools-container">
            {message.data.cards.map((card) => (
              <ToolCard key={card.id} tool={card} />
            ))}
          </div>
        )}
      </div>
    ))}
  </div>
); 

// In your API route
export async function POST(req: Request) {
  // ... existing code to find relevant tools

  const data = createStreamableData();
  
  // Add the tools to the data stream
  data.append({ cards: relevantTools });

  // Generate and stream the text response
  const textStream = await generateText({
    model: 'your-model',
    prompt: constructedPrompt,
    // other options
  });

  // Return both the text stream and the data
  return StreamingTextResponse.withData(textStream, data);
} 